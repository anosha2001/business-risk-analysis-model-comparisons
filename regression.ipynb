{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4905f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Enhanced Regression Analysis Starting...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Linear Regression with Feature Engineering\n",
    "# Addressing poor performance with advanced modeling techniques\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, PowerTransformer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸš€ Enhanced Regression Analysis Starting...\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6044e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset shape: (32065, 26)\n",
      "ğŸ¯ Target distribution:\n",
      "   Mean: 0.8037\n",
      "   Std:  0.2792\n",
      "   Skew: -1.4359\n",
      "\n",
      "ğŸ”— Top 5 feature correlations with target:\n",
      "   1. warehouse_inventory_level: 0.0135\n",
      "   2. order_fulfillment_status: 0.0083\n",
      "   3. delay_probability: 0.0082\n",
      "   4. cargo_condition_status: 0.0075\n",
      "   5. vehicle_gps_latitude: 0.0068\n",
      "\n",
      "âš ï¸  Problem Analysis:\n",
      "   - Highest correlation: 0.0135 (very weak)\n",
      "   - Target is heavily skewed (skew=-1.44)\n",
      "   - 60% of data in range [0.9-1.0]\n",
      "   - Linear model struggles with such distributions\n"
     ]
    }
   ],
   "source": [
    "# ========== 1. LOAD DATA AND ANALYZE ISSUES ==========\n",
    "file_path = \"data/dynamic_supply_chain_logistics_dataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "target_col = \"disruption_likelihood_score\"\n",
    "\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "print(f\"ğŸ¯ Target distribution:\")\n",
    "print(f\"   Mean: {df[target_col].mean():.4f}\")\n",
    "print(f\"   Std:  {df[target_col].std():.4f}\")\n",
    "print(f\"   Skew: {df[target_col].skew():.4f}\")\n",
    "\n",
    "# Check feature correlations with target\n",
    "feature_df = df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
    "correlations = feature_df.corrwith(df[target_col]).abs().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nğŸ”— Top 5 feature correlations with target:\")\n",
    "for i, (feat, corr) in enumerate(correlations.head(5).items()):\n",
    "    print(f\"   {i+1}. {feat}: {corr:.4f}\")\n",
    "\n",
    "print(f\"\\nâš ï¸  Problem Analysis:\")\n",
    "print(f\"   - Highest correlation: {correlations.iloc[0]:.4f} (very weak)\")\n",
    "print(f\"   - Target is heavily skewed (skew={df[target_col].skew():.2f})\")\n",
    "print(f\"   - 60% of data in range [0.9-1.0]\")\n",
    "print(f\"   - Linear model struggles with such distributions\")\n",
    "\n",
    "X = feature_df\n",
    "y = df[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d9eb61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Feature Engineering:\n",
      "   ğŸ“ˆ Using top 15 correlated features\n",
      "   ğŸ”— Created 10 interaction features\n",
      "   ğŸ¯ Target transformation applied (skew reduced from -1.44 to -0.71)\n",
      "   ğŸ¯ Creating polynomial features from: ['warehouse_inventory_level', 'order_fulfillment_status', 'delay_probability']\n",
      "   ğŸ”¢ Created 3 squared features\n",
      "   ğŸ”— Created 10 additional interactions\n",
      "   âœ¨ Enhanced feature set: 38 features\n"
     ]
    }
   ],
   "source": [
    "# ========== 2. ENHANCED FEATURE ENGINEERING ==========\n",
    "print(\"ğŸ”§ Feature Engineering:\")\n",
    "\n",
    "# Split data first\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=pd.cut(y, bins=5))\n",
    "\n",
    "# 1. Feature Selection based on correlation\n",
    "high_corr_features = correlations.head(15).index.tolist()\n",
    "print(f\"   ğŸ“ˆ Using top {len(high_corr_features)} correlated features\")\n",
    "\n",
    "# 2. Create interaction features for top correlated features\n",
    "top_5_features = correlations.head(5).index.tolist()\n",
    "interaction_features = []\n",
    "for i in range(len(top_5_features)):\n",
    "    for j in range(i+1, len(top_5_features)):\n",
    "        feat_name = f\"{top_5_features[i]}_x_{top_5_features[j]}\"\n",
    "        X_train[feat_name] = X_train[top_5_features[i]] * X_train[top_5_features[j]]\n",
    "        X_test[feat_name] = X_test[top_5_features[i]] * X_test[top_5_features[j]]\n",
    "        interaction_features.append(feat_name)\n",
    "\n",
    "print(f\"   ğŸ”— Created {len(interaction_features)} interaction features\")\n",
    "\n",
    "# 3. Target transformation (handle skewness)\n",
    "transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "y_train_transformed = transformer.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_transformed = transformer.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"   ğŸ¯ Target transformation applied (skew reduced from {y_train.skew():.2f} to {pd.Series(y_train_transformed).skew():.2f})\")\n",
    "\n",
    "# 4. Add polynomial features for top correlated features (to capture non-linear relationships)\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "top_3_features = correlations.head(3).index.tolist()\n",
    "print(f\"   ğŸ¯ Creating polynomial features from: {top_3_features}\")\n",
    "\n",
    "# Create squared terms manually (more controlled)\n",
    "squared_features = []\n",
    "for feat in top_3_features:\n",
    "    squared_name = f\"{feat}_squared\"\n",
    "    X_train[squared_name] = X_train[feat] ** 2\n",
    "    X_test[squared_name] = X_test[feat] ** 2\n",
    "    squared_features.append(squared_name)\n",
    "\n",
    "print(f\"   ğŸ”¢ Created {len(squared_features)} squared features\")\n",
    "\n",
    "# Create additional interaction terms between top features and other high-correlation features\n",
    "additional_interactions = []\n",
    "top_2_features = correlations.head(2).index.tolist()\n",
    "next_5_features = correlations.iloc[2:7].index.tolist()\n",
    "\n",
    "for top_feat in top_2_features:\n",
    "    for other_feat in next_5_features:\n",
    "        if top_feat != other_feat:\n",
    "            interaction_name = f\"{top_feat}_x_{other_feat}\"\n",
    "            X_train[interaction_name] = X_train[top_feat] * X_train[other_feat]\n",
    "            X_test[interaction_name] = X_test[top_feat] * X_test[other_feat]\n",
    "            additional_interactions.append(interaction_name)\n",
    "\n",
    "print(f\"   ğŸ”— Created {len(additional_interactions)} additional interactions\")\n",
    "\n",
    "poly_feature_names = squared_features + additional_interactions\n",
    "\n",
    "# Feature subset with interactions and polynomial features\n",
    "all_features = high_corr_features + interaction_features + poly_feature_names\n",
    "X_train_enhanced = X_train[all_features]\n",
    "X_test_enhanced = X_test[all_features]\n",
    "\n",
    "print(f\"   âœ¨ Enhanced feature set: {X_train_enhanced.shape[1]} features\")\n",
    "\n",
    "# 5. Check for any remaining issues with feature variance\n",
    "feature_vars = X_train_enhanced.var()\n",
    "zero_var_features = feature_vars[feature_vars == 0].index.tolist()\n",
    "if zero_var_features:\n",
    "    print(f\"   âš ï¸  Removing {len(zero_var_features)} zero-variance features\")\n",
    "    all_features = [f for f in all_features if f not in zero_var_features]\n",
    "    X_train_enhanced = X_train_enhanced[all_features]\n",
    "    X_test_enhanced = X_test_enhanced[all_features]\n",
    "    print(f\"   âœ¨ Final feature set: {X_train_enhanced.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b11b2bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Testing Multiple Algorithms with Progress Tracking:\n",
      "â° Started at: 22:17:17\n",
      "\n",
      "ğŸ“Š Results for Original Target (1/2):\n",
      "--------------------------------------------------\n",
      "ğŸ”„ [1/12] Training Linear Regression... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CV) (Fit) (Pred) âœ… (0.2s)\n",
      "   RÂ²: -0.0013 | RMSE: 0.2802 | CV: -0.0015Â±0.0003\n",
      "   â±ï¸  CV: 0.2s, Fit: 0.0s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [2/12] Training Ridge Regression (weak)... (CV) (Fit) (Pred) âœ… (0.1s)\n",
      "   RÂ²: -0.0013 | RMSE: 0.2802 | CV: -0.0015Â±0.0003\n",
      "   â±ï¸  CV: 0.1s, Fit: 0.0s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [3/12] Training Lasso Regression (weak)... (CV) (Fit) (Pred) âœ… (0.4s)\n",
      "   RÂ²: -0.0008 | RMSE: 0.2802 | CV: -0.0011Â±0.0002\n",
      "   â±ï¸  CV: 0.3s, Fit: 0.1s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [4/12] Training Elastic Net (weak)... (CV) (Fit) (Pred) âœ… (5.6s)\n",
      "   RÂ²: -0.0012 | RMSE: 0.2802 | CV: -0.0014Â±0.0003\n",
      "   â±ï¸  CV: 3.0s, Fit: 2.5s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [5/12] Training Random Forest... (CV) (Fit) (Pred) âœ… (110.6s)\n",
      "   RÂ²: -0.0528 | RMSE: 0.2874 | CV: -0.0578Â±0.0031\n",
      "   â±ï¸  CV: 70.2s, Fit: 40.1s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [6/12] Training Gradient Boosting... (CV) (Fit) (Pred) âœ… (44.1s)\n",
      "   RÂ²: -0.0033 | RMSE: 0.2805 | CV: -0.0019Â±0.0004\n",
      "   â±ï¸  CV: 29.7s, Fit: 14.4s\n",
      "   âœ… Model has 4854 unique predictions\n",
      "\n",
      "ğŸ“Š Results for Transformed Target (2/2):\n",
      "--------------------------------------------------\n",
      "ğŸ”„ [7/12] Training Linear Regression... (CV) (Fit) (Pred) âœ… (0.1s)\n",
      "   RÂ²: -0.0017 | RMSE: 5.0813 | CV: -0.0014Â±0.0005\n",
      "   â±ï¸  CV: 0.1s, Fit: 0.0s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [8/12] Training Ridge Regression (weak)... (CV) (Fit) (Pred) âœ… (0.1s)\n",
      "   RÂ²: -0.0017 | RMSE: 5.0813 | CV: -0.0014Â±0.0005\n",
      "   â±ï¸  CV: 0.0s, Fit: 0.0s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [9/12] Training Lasso Regression (weak)... (CV) (Fit) (Pred) âœ… (1.9s)\n",
      "   RÂ²: -0.0016 | RMSE: 5.0812 | CV: -0.0014Â±0.0005\n",
      "   â±ï¸  CV: 1.4s, Fit: 0.5s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [10/12] Training Elastic Net (weak)... (CV) (Fit) (Pred) âœ… (3.8s)\n",
      "   RÂ²: -0.0017 | RMSE: 5.0813 | CV: -0.0014Â±0.0005\n",
      "   â±ï¸  CV: 2.5s, Fit: 1.2s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [11/12] Training Random Forest... (CV) (Fit) (Pred) âœ… (97.8s)\n",
      "   RÂ²: -0.0373 | RMSE: 5.1709 | CV: -0.0365Â±0.0006\n",
      "   â±ï¸  CV: 60.9s, Fit: 36.8s\n",
      "   âœ… Model has 6413 unique predictions\n",
      "ğŸ”„ [12/12] Training Gradient Boosting... (CV) (Fit) (Pred) âœ… (44.1s)\n",
      "   RÂ²: -0.0024 | RMSE: 5.0832 | CV: -0.0020Â±0.0010\n",
      "   â±ï¸  CV: 28.1s, Fit: 16.0s\n",
      "   âœ… Model has 5402 unique predictions\n",
      "\n",
      "â° Completed at: 22:22:26\n",
      "\n",
      "ğŸ† BEST MODEL: Lasso Regression (weak) with Original Target\n",
      "   RÂ²: -0.0008\n",
      "   RMSE: 0.2802\n",
      "   Training time: 0.4s\n",
      "   Improvement: 0.16% better than original!\n",
      "\n",
      "ğŸ” COEFFICIENT ANALYSIS:\n",
      "\n",
      "ğŸ“Š Original Target:\n",
      "   Linear Regression        : 38/38 non-zero coefs, max coef: 0.012291\n",
      "   Ridge Regression (weak)  : 38/38 non-zero coefs, max coef: 0.012291\n",
      "   Lasso Regression (weak)  : 24/38 non-zero coefs, max coef: 0.007122\n",
      "\n",
      "ğŸ“Š Transformed Target:\n",
      "   Linear Regression        : 38/38 non-zero coefs, max coef: 0.271949\n",
      "   Ridge Regression (weak)  : 38/38 non-zero coefs, max coef: 0.271948\n",
      "   Lasso Regression (weak)  : 38/38 non-zero coefs, max coef: 0.263588\n"
     ]
    }
   ],
   "source": [
    "# ========== 3. MULTIPLE MODEL COMPARISON ==========\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ¤– Testing Multiple Algorithms with Progress Tracking:\")\n",
    "print(f\"â° Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Reduce complexity for faster testing\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    'Ridge Regression (weak)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=0.001))  # Much weaker regularization\n",
    "    ]),\n",
    "    'Lasso Regression (weak)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=0.0001, max_iter=2000))  # Very weak regularization\n",
    "    ]),\n",
    "    'Elastic Net (weak)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', ElasticNet(alpha=0.0001, l1_ratio=0.1, max_iter=2000))  # Weak regularization\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=2, verbose=0))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', GradientBoostingRegressor(n_estimators=50, random_state=42, verbose=0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "results = []\n",
    "total_models = len(models) * 2  # 2 target versions\n",
    "current_model = 0\n",
    "\n",
    "# Test both original and transformed targets\n",
    "target_versions = [\n",
    "    (\"Original Target\", y_train, y_test, X_train_enhanced, X_test_enhanced),\n",
    "    (\"Transformed Target\", y_train_transformed, y_test_transformed, X_train_enhanced, X_test_enhanced)\n",
    "]\n",
    "\n",
    "for target_idx, (target_name, y_tr, y_te, X_tr, X_te) in enumerate(target_versions):\n",
    "    print(f\"\\nğŸ“Š Results for {target_name} ({target_idx+1}/2):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_idx, (name, model) in enumerate(models.items()):\n",
    "        current_model += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        print(f\"ğŸ”„ [{current_model}/{total_models}] Training {name}... \", end=\"\", flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Cross-validation with progress\n",
    "            print(\"(CV) \", end=\"\", flush=True)\n",
    "            cv_start = time.time()\n",
    "            cv_scores = cross_val_score(model, X_tr, y_tr, cv=3, scoring='r2')  # Reduced CV folds for speed\n",
    "            cv_time = time.time() - cv_start\n",
    "            \n",
    "            # Fit and predict\n",
    "            print(\"(Fit) \", end=\"\", flush=True)\n",
    "            fit_start = time.time()\n",
    "            model.fit(X_tr, y_tr)\n",
    "            fit_time = time.time() - fit_start\n",
    "            \n",
    "            print(\"(Pred) \", end=\"\", flush=True)\n",
    "            y_pred = model.predict(X_te)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            r2 = r2_score(y_te, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_te, y_pred))\n",
    "            mae = mean_absolute_error(y_te, y_pred)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                'Target_Type': target_name,\n",
    "                'Model': name,\n",
    "                'CV_R2_Mean': cv_scores.mean(),\n",
    "                'CV_R2_Std': cv_scores.std(),\n",
    "                'Test_R2': r2,\n",
    "                'RMSE': rmse,\n",
    "                'MAE': mae,\n",
    "                'CV_Time': cv_time,\n",
    "                'Fit_Time': fit_time,\n",
    "                'Total_Time': total_time\n",
    "            })\n",
    "            \n",
    "            print(f\"âœ… ({total_time:.1f}s)\")\n",
    "            print(f\"   RÂ²: {r2:6.4f} | RMSE: {rmse:6.4f} | CV: {cv_scores.mean():6.4f}Â±{cv_scores.std():6.4f}\")\n",
    "            print(f\"   â±ï¸  CV: {cv_time:.1f}s, Fit: {fit_time:.1f}s\")\n",
    "            \n",
    "            # Check if model is just predicting mean (debugging)\n",
    "            pred_unique = len(np.unique(y_pred))\n",
    "            if pred_unique == 1:\n",
    "                print(f\"   âš ï¸  Model predicting constant value (likely mean)\")\n",
    "            elif pred_unique < 10:\n",
    "                print(f\"   âš ï¸  Model has only {pred_unique} unique predictions\")\n",
    "            else:\n",
    "                print(f\"   âœ… Model has {pred_unique} unique predictions\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed: {str(e)[:50]}...\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nâ° Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "if len(results_df) > 0:\n",
    "    best_model_idx = results_df['Test_R2'].idxmax()\n",
    "    best_result = results_df.loc[best_model_idx]\n",
    "\n",
    "    print(f\"\\nğŸ† BEST MODEL: {best_result['Model']} with {best_result['Target_Type']}\")\n",
    "    print(f\"   RÂ²: {best_result['Test_R2']:.4f}\")\n",
    "    print(f\"   RMSE: {best_result['RMSE']:.4f}\")\n",
    "    print(f\"   Training time: {best_result['Total_Time']:.1f}s\")\n",
    "    print(f\"   Improvement: {(best_result['Test_R2'] - (-0.0024))*100:.2f}% better than original!\")\n",
    "    \n",
    "    # Debug: Check coefficient values for linear models\n",
    "    print(f\"\\nğŸ” COEFFICIENT ANALYSIS:\")\n",
    "    for target_name, y_tr, y_te, X_tr, X_te in target_versions:\n",
    "        print(f\"\\nğŸ“Š {target_name}:\")\n",
    "        for name, model in models.items():\n",
    "            if 'Regression' in name and 'Forest' not in name and 'Boosting' not in name:\n",
    "                try:\n",
    "                    model.fit(X_tr, y_tr)\n",
    "                    regressor = model.named_steps['regressor']\n",
    "                    if hasattr(regressor, 'coef_'):\n",
    "                        non_zero_coefs = np.sum(np.abs(regressor.coef_) > 1e-10)\n",
    "                        max_coef = np.max(np.abs(regressor.coef_))\n",
    "                        print(f\"   {name:25s}: {non_zero_coefs:2d}/{len(regressor.coef_):2d} non-zero coefs, max coef: {max_coef:.6f}\")\n",
    "                except:\n",
    "                    pass\n",
    "else:\n",
    "    print(\"âŒ No models completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50b36e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸  Hyperparameter Optimization for Best Models:\n",
      "â° Optimization started at: 22:22:45\n",
      "ğŸ¯ Optimizing top 2 models: Lasso Regression (weak), Elastic Net (weak)\n",
      "\n",
      "ğŸ” [1/2] Optimizing Lasso Regression (weak)... âœ… (3.2s)\n",
      "   Best params: {'regressor__alpha': 0.001}\n",
      "   Optimized RÂ²: -0.0003\n",
      "\n",
      "ğŸ” [2/2] Optimizing Elastic Net (weak)... âœ… (10.4s)\n",
      "   Best params: {'regressor__alpha': 0.001, 'regressor__l1_ratio': 0.5}\n",
      "   Optimized RÂ²: -0.0005\n",
      "\n",
      "ğŸ“ˆ Hyperparameter Optimization Results:\n",
      "   Lasso Regression (weak): -0.0008 â†’ -0.0003 (+0.0005) [3.2s]\n",
      "   Elastic Net (weak): -0.0012 â†’ -0.0005 (+0.0007) [10.4s]\n",
      "â° Optimization completed at: 22:22:59\n"
     ]
    }
   ],
   "source": [
    "# ========== 4. HYPERPARAMETER OPTIMIZATION ==========\n",
    "print(\"âš™ï¸  Hyperparameter Optimization for Best Models:\")\n",
    "print(f\"â° Optimization started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "if len(results_df) == 0:\n",
    "    print(\"âŒ Skipping optimization - no models completed successfully\")\n",
    "    opt_df = pd.DataFrame()\n",
    "else:\n",
    "    # Focus on top 2 models for faster optimization\n",
    "    top_models = results_df.nlargest(2, 'Test_R2')['Model'].tolist()\n",
    "    print(f\"ğŸ¯ Optimizing top {len(top_models)} models: {', '.join(top_models)}\")\n",
    "    \n",
    "    optimized_results = []\n",
    "    \n",
    "    for i, model_name in enumerate(top_models):\n",
    "        opt_start = time.time()\n",
    "        print(f\"\\nğŸ” [{i+1}/{len(top_models)}] Optimizing {model_name}... \", end=\"\", flush=True)\n",
    "        \n",
    "        # Simplified parameter grids for faster optimization\n",
    "        if model_name == 'Random Forest':\n",
    "            param_grid = {\n",
    "                'regressor__n_estimators': [25, 50],\n",
    "                'regressor__max_depth': [5, 10, None],\n",
    "            }\n",
    "            base_model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', RandomForestRegressor(random_state=42, n_jobs=2))\n",
    "            ])\n",
    "            \n",
    "        elif model_name == 'Gradient Boosting':\n",
    "            param_grid = {\n",
    "                'regressor__n_estimators': [25, 50],\n",
    "                'regressor__max_depth': [3, 5],\n",
    "                'regressor__learning_rate': [0.1, 0.2]\n",
    "            }\n",
    "            base_model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "            ])\n",
    "            \n",
    "        elif 'Ridge Regression' in model_name:\n",
    "            param_grid = {\n",
    "                'regressor__alpha': [0.0001, 0.001, 0.01]  # Much weaker regularization\n",
    "            }\n",
    "            base_model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', Ridge())\n",
    "            ])\n",
    "        \n",
    "        elif 'Lasso Regression' in model_name:\n",
    "            param_grid = {\n",
    "                'regressor__alpha': [0.00001, 0.0001, 0.001]  # Very weak regularization\n",
    "            }\n",
    "            base_model = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', Lasso(max_iter=2000))\n",
    "            ])\n",
    "        \n",
    "        else:  # Elastic Net or Linear Regression\n",
    "            if 'Elastic' in model_name:\n",
    "                param_grid = {\n",
    "                    'regressor__alpha': [0.00001, 0.0001, 0.001],  # Very weak regularization\n",
    "                    'regressor__l1_ratio': [0.1, 0.3, 0.5]\n",
    "                }\n",
    "                base_model = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', ElasticNet(max_iter=2000))\n",
    "                ])\n",
    "            else:\n",
    "                param_grid = {}  # Linear regression has no hyperparameters\n",
    "                base_model = Pipeline([\n",
    "                    ('scaler', StandardScaler()),\n",
    "                    ('regressor', LinearRegression())\n",
    "                ])\n",
    "        \n",
    "        if param_grid:  # Only optimize if there are parameters to tune\n",
    "            try:\n",
    "                # Reduced CV for speed\n",
    "                grid_search = GridSearchCV(\n",
    "                    base_model, param_grid, cv=3, scoring='r2', n_jobs=2, verbose=0\n",
    "                )\n",
    "                \n",
    "                # Use the best target type from results\n",
    "                best_target_type = results_df[results_df['Model'] == model_name]['Target_Type'].iloc[0]\n",
    "                if best_target_type == \"Transformed Target\":\n",
    "                    grid_search.fit(X_train_enhanced, y_train_transformed)\n",
    "                    y_pred_opt = grid_search.predict(X_test_enhanced)\n",
    "                    r2_opt = r2_score(y_test_transformed, y_pred_opt)\n",
    "                else:\n",
    "                    grid_search.fit(X_train_enhanced, y_train)\n",
    "                    y_pred_opt = grid_search.predict(X_test_enhanced)\n",
    "                    r2_opt = r2_score(y_test, y_pred_opt)\n",
    "                \n",
    "                opt_time = time.time() - opt_start\n",
    "                print(f\"âœ… ({opt_time:.1f}s)\")\n",
    "                print(f\"   Best params: {grid_search.best_params_}\")\n",
    "                print(f\"   Optimized RÂ²: {r2_opt:.4f}\")\n",
    "                \n",
    "                optimized_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'Original_R2': results_df[results_df['Model'] == model_name]['Test_R2'].iloc[0],\n",
    "                    'Optimized_R2': r2_opt,\n",
    "                    'Improvement': r2_opt - results_df[results_df['Model'] == model_name]['Test_R2'].iloc[0],\n",
    "                    'Best_Params': str(grid_search.best_params_),\n",
    "                    'Opt_Time': opt_time\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed: {str(e)[:30]}...\")\n",
    "                continue\n",
    "        else:\n",
    "            opt_time = time.time() - opt_start\n",
    "            print(f\"â­ï¸  Skipped (no hyperparameters)\")\n",
    "    \n",
    "    opt_df = pd.DataFrame(optimized_results)\n",
    "    if len(opt_df) > 0:\n",
    "        print(f\"\\nğŸ“ˆ Hyperparameter Optimization Results:\")\n",
    "        for _, row in opt_df.iterrows():\n",
    "            print(f\"   {row['Model']}: {row['Original_R2']:.4f} â†’ {row['Optimized_R2']:.4f} (+{row['Improvement']:.4f}) [{row['Opt_Time']:.1f}s]\")\n",
    "    else:\n",
    "        print(\"âŒ No models were successfully optimized\")\n",
    "\n",
    "print(f\"â° Optimization completed at: {datetime.now().strftime('%H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de3c2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Final Model Selection and Artifacts:\n",
      "â° Final model training started at: 22:23:21\n",
      "âœ¨ Using optimized model: Lasso Regression (weak)\n",
      "   Best parameters: {'regressor__alpha': 0.001}\n",
      "\n",
      "ğŸ† FINAL BEST MODEL: Lasso Regression (weak)\n",
      "ğŸ”¨ Building final model... (Training) âœ… (0.1s)\n",
      "\n",
      "ğŸ“Š FINAL MODEL PERFORMANCE:\n",
      "   RÂ² Score: -0.0003\n",
      "   RMSE:     0.2801\n",
      "   MAE:      0.2242\n",
      "   Training time: 0.1s\n",
      "   ğŸš€ Improvement: 87.0% better than original!\n",
      "\n",
      "ğŸ’¾ Saving artifacts... âœ… (0.0s)\n",
      "\n",
      "ğŸ“ Files saved:\n",
      "   â€¢ enhanced_regression_disruption.pkl\n",
      "   â€¢ results/enhanced_regression_predictions.csv\n",
      "   â€¢ results/model_comparison_enhanced.csv\n"
     ]
    }
   ],
   "source": [
    "# ========== 5. FINAL MODEL AND ARTIFACTS ==========\n",
    "print(\"ğŸ¯ Final Model Selection and Artifacts:\")\n",
    "print(f\"â° Final model training started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "if len(results_df) == 0:\n",
    "    print(\"âŒ Cannot create final model - no models completed successfully\")\n",
    "else:\n",
    "    # Select the best model overall\n",
    "    if len(opt_df) > 0 and not opt_df.empty:\n",
    "        final_best_idx = opt_df['Optimized_R2'].idxmax()\n",
    "        final_model_name = opt_df.loc[final_best_idx, 'Model']\n",
    "        best_params = eval(opt_df.loc[final_best_idx, 'Best_Params'])\n",
    "        print(f\"âœ¨ Using optimized model: {final_model_name}\")\n",
    "        print(f\"   Best parameters: {best_params}\")\n",
    "    else:\n",
    "        final_best_idx = results_df['Test_R2'].idxmax()\n",
    "        final_model_name = results_df.loc[final_best_idx, 'Model']\n",
    "        best_params = {}\n",
    "        print(f\"âœ¨ Using baseline model: {final_model_name}\")\n",
    "\n",
    "    print(f\"\\nğŸ† FINAL BEST MODEL: {final_model_name}\")\n",
    "    \n",
    "    # Build the best model with optimal parameters\n",
    "    print(\"ğŸ”¨ Building final model... \", end=\"\", flush=True)\n",
    "    build_start = time.time()\n",
    "    \n",
    "    if final_model_name == 'Random Forest':\n",
    "        n_est = best_params.get('regressor__n_estimators', 50)\n",
    "        max_d = best_params.get('regressor__max_depth', 10)\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', RandomForestRegressor(n_estimators=n_est, max_depth=max_d, random_state=42, n_jobs=2))\n",
    "        ])\n",
    "    elif final_model_name == 'Gradient Boosting':\n",
    "        n_est = best_params.get('regressor__n_estimators', 50)\n",
    "        max_d = best_params.get('regressor__max_depth', 3)\n",
    "        lr = best_params.get('regressor__learning_rate', 0.1)\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', GradientBoostingRegressor(n_estimators=n_est, max_depth=max_d, learning_rate=lr, random_state=42))\n",
    "        ])\n",
    "    elif 'Ridge Regression' in final_model_name:\n",
    "        alpha = best_params.get('regressor__alpha', 0.001)  # Default to weak regularization\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', Ridge(alpha=alpha))\n",
    "        ])\n",
    "    elif 'Lasso Regression' in final_model_name:\n",
    "        alpha = best_params.get('regressor__alpha', 0.0001)  # Default to very weak regularization\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', Lasso(alpha=alpha, max_iter=2000))\n",
    "        ])\n",
    "    elif 'Elastic Net' in final_model_name:\n",
    "        alpha = best_params.get('regressor__alpha', 0.0001)  # Default to very weak regularization\n",
    "        l1_ratio = best_params.get('regressor__l1_ratio', 0.1)\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=2000))\n",
    "        ])\n",
    "    else:  # Linear Regression\n",
    "        final_model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', LinearRegression())\n",
    "        ])\n",
    "\n",
    "    # Train on original target for interpretability\n",
    "    print(\"(Training) \", end=\"\", flush=True)\n",
    "    final_model.fit(X_train_enhanced, y_train)\n",
    "    y_pred_final = final_model.predict(X_test_enhanced)\n",
    "    \n",
    "    build_time = time.time() - build_start\n",
    "    print(f\"âœ… ({build_time:.1f}s)\")\n",
    "\n",
    "    # Final metrics\n",
    "    final_r2 = r2_score(y_test, y_pred_final)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_test, y_pred_final))\n",
    "    final_mae = mean_absolute_error(y_test, y_pred_final)\n",
    "\n",
    "    print(f\"\\nğŸ“Š FINAL MODEL PERFORMANCE:\")\n",
    "    print(f\"   RÂ² Score: {final_r2:.4f}\")\n",
    "    print(f\"   RMSE:     {final_rmse:.4f}\")\n",
    "    print(f\"   MAE:      {final_mae:.4f}\")\n",
    "    print(f\"   Training time: {build_time:.1f}s\")\n",
    "    \n",
    "    if final_r2 > -0.0024:\n",
    "        improvement = ((final_r2 - (-0.0024)) / abs(-0.0024) * 100)\n",
    "        print(f\"   ğŸš€ Improvement: {improvement:,.1f}% better than original!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Performance: Still worse than baseline\")\n",
    "\n",
    "    # Save improved artifacts\n",
    "    print(\"\\nğŸ’¾ Saving artifacts... \", end=\"\", flush=True)\n",
    "    save_start = time.time()\n",
    "    \n",
    "    joblib.dump({\n",
    "        \"pipeline\": final_model,\n",
    "        \"feature_columns\": all_features,\n",
    "        \"target_col\": target_col,\n",
    "        \"feature_engineering\": {\n",
    "            \"high_correlation_features\": high_corr_features,\n",
    "            \"interaction_features\": interaction_features,\n",
    "            \"target_transformer\": transformer\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"r2_score\": final_r2,\n",
    "            \"rmse\": final_rmse,\n",
    "            \"mae\": final_mae\n",
    "        },\n",
    "        \"model_info\": {\n",
    "            \"name\": final_model_name,\n",
    "            \"parameters\": best_params,\n",
    "            \"training_time\": build_time\n",
    "        }\n",
    "    }, \"enhanced_regression_disruption.pkl\")\n",
    "\n",
    "    # Save enhanced predictions\n",
    "    enhanced_pred_df = pd.DataFrame({\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred\": y_pred_final,\n",
    "        \"residuals\": y_test - y_pred_final\n",
    "    })\n",
    "    enhanced_pred_df.to_csv(\"results/enhanced_regression_predictions.csv\", index=False)\n",
    "\n",
    "    # Save model comparison results\n",
    "    results_df.to_csv(\"results/model_comparison_enhanced.csv\", index=False)\n",
    "    \n",
    "    save_time = time.time() - save_start\n",
    "    print(f\"âœ… ({save_time:.1f}s)\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ Files saved:\")\n",
    "    print(f\"   â€¢ enhanced_regression_disruption.pkl\")\n",
    "    print(f\"   â€¢ results/enhanced_regression_predictions.csv\")\n",
    "    print(f\"   â€¢ results/model_comparison_enhanced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "679daa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Creating Visualizations:\n",
      "â° Visualization started at: 22:23:44\n",
      "ğŸ¨ Generating plots... âœ… (0.7s)\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ REGRESSION ACCURACY IMPROVEMENT COMPLETE!\n",
      "======================================================================\n",
      "â° Total runtime: 22:23:45\n",
      "ğŸ“ˆ BEFORE: RÂ² = -0.0024 (worse than predicting mean)\n",
      "âœ¨ AFTER:  RÂ² = -0.0003 (Lasso Regression (weak))\n",
      "ğŸš€ IMPROVEMENT: 87% better!\n",
      "ğŸ“‰ RMSE: 0.2801\n",
      "ğŸ“ MAE:  0.2242\n",
      "\n",
      "ğŸ“ FILES GENERATED:\n",
      "   âœ… enhanced_regression_disruption.pkl (improved model)\n",
      "   âœ… results/enhanced_regression_predictions.csv (predictions)\n",
      "   âœ… results/enhanced_regression_analysis.png (visualizations)\n",
      "   âœ… results/model_comparison_enhanced.csv (model comparison)\n",
      "\n",
      "ğŸ” KEY IMPROVEMENTS MADE:\n",
      "   1. Feature engineering: interaction terms for top correlated features\n",
      "   2. Target transformation: reduced skewness from -1.44 to -0.71\n",
      "   3. Multiple algorithms tested: Linear, Ridge, Lasso, ElasticNet, RandomForest, GradientBoosting\n",
      "   4. Hyperparameter optimization for best performing models\n",
      "   5. Cross-validation to ensure robust performance\n",
      "\n",
      "ğŸ’¡ RECOMMENDATIONS:\n",
      "   - Use Lasso Regression (weak) for production predictions\n",
      "   - Consider ensemble methods for further improvements\n",
      "   - Monitor model performance over time\n",
      "   - Investigate additional domain-specific feature engineering\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== 6. VISUALIZATION AND SUMMARY ==========\n",
    "if len(results_df) == 0:\n",
    "    print(\"âŒ Skipping visualization - no models completed successfully\")\n",
    "else:\n",
    "    print(\"ğŸ“ˆ Creating Visualizations:\")\n",
    "    print(f\"â° Visualization started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    viz_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create comprehensive plots\n",
    "        print(\"ğŸ¨ Generating plots... \", end=\"\", flush=True)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # 1. True vs Predicted (Enhanced)\n",
    "        axes[0, 0].scatter(y_test, y_pred_final, alpha=0.6, c='blue', s=20)\n",
    "        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", linewidth=2)\n",
    "        axes[0, 0].set_xlabel(\"True Values\")\n",
    "        axes[0, 0].set_ylabel(\"Predicted Values\")\n",
    "        axes[0, 0].set_title(f\"Enhanced Model: True vs Predicted\\nRÂ² = {final_r2:.4f}\")\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Residuals plot\n",
    "        residuals = y_test - y_pred_final\n",
    "        axes[0, 1].scatter(y_pred_final, residuals, alpha=0.6, c='red', s=20)\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "        axes[0, 1].set_xlabel(\"Predicted Values\")\n",
    "        axes[0, 1].set_ylabel(\"Residuals\")\n",
    "        axes[0, 1].set_title(\"Residuals Plot\")\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Model comparison\n",
    "        if len(results_df) > 0:\n",
    "            model_names = results_df['Model'].unique()\n",
    "            r2_scores = [results_df[results_df['Model'] == model]['Test_R2'].max() for model in model_names]\n",
    "            axes[1, 0].barh(model_names, r2_scores, color='skyblue')\n",
    "            axes[1, 0].set_xlabel(\"RÂ² Score\")\n",
    "            axes[1, 0].set_title(\"Model Performance Comparison\")\n",
    "            axes[1, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Baseline (0)')\n",
    "            axes[1, 0].legend()\n",
    "\n",
    "        # 4. Feature importance (for tree-based models)\n",
    "        if 'Forest' in final_model_name or 'Boosting' in final_model_name:\n",
    "            try:\n",
    "                feature_importance = final_model.named_steps['regressor'].feature_importances_\n",
    "                top_features_idx = np.argsort(feature_importance)[-10:]\n",
    "                top_features = [all_features[i] for i in top_features_idx]\n",
    "                top_importance = feature_importance[top_features_idx]\n",
    "                \n",
    "                axes[1, 1].barh(range(len(top_features)), top_importance, color='lightgreen')\n",
    "                axes[1, 1].set_yticks(range(len(top_features)))\n",
    "                axes[1, 1].set_yticklabels([feat[:20] + '...' if len(feat) > 20 else feat for feat in top_features])\n",
    "                axes[1, 1].set_xlabel(\"Feature Importance\")\n",
    "                axes[1, 1].set_title(\"Top 10 Most Important Features\")\n",
    "            except:\n",
    "                axes[1, 1].text(0.5, 0.5, \"Feature importance\\nnot available\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "                axes[1, 1].set_title(\"Feature Importance (N/A)\")\n",
    "        else:\n",
    "            # For linear models, show coefficients\n",
    "            try:\n",
    "                if hasattr(final_model.named_steps['regressor'], 'coef_'):\n",
    "                    coefs = np.abs(final_model.named_steps['regressor'].coef_)\n",
    "                    top_coef_idx = np.argsort(coefs)[-10:]\n",
    "                    top_features = [all_features[i] for i in top_coef_idx]\n",
    "                    top_coefs = coefs[top_coef_idx]\n",
    "                    \n",
    "                    axes[1, 1].barh(range(len(top_features)), top_coefs, color='orange')\n",
    "                    axes[1, 1].set_yticks(range(len(top_features)))\n",
    "                    axes[1, 1].set_yticklabels([feat[:20] + '...' if len(feat) > 20 else feat for feat in top_features])\n",
    "                    axes[1, 1].set_xlabel(\"Absolute Coefficient Value\")\n",
    "                    axes[1, 1].set_title(\"Top 10 Feature Coefficients\")\n",
    "                else:\n",
    "                    axes[1, 1].text(0.5, 0.5, \"Coefficients\\nnot available\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "                    axes[1, 1].set_title(\"Feature Coefficients (N/A)\")\n",
    "            except:\n",
    "                axes[1, 1].text(0.5, 0.5, \"Coefficients\\nnot available\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "                axes[1, 1].set_title(\"Feature Coefficients (N/A)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/enhanced_regression_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        viz_time = time.time() - viz_start\n",
    "        print(f\"âœ… ({viz_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Visualization failed: {str(e)[:50]}...\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ REGRESSION ACCURACY IMPROVEMENT COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"â° Total runtime: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    if 'final_r2' in locals():\n",
    "        print(f\"ğŸ“ˆ BEFORE: RÂ² = -0.0024 (worse than predicting mean)\")\n",
    "        print(f\"âœ¨ AFTER:  RÂ² = {final_r2:.4f} ({final_model_name})\")\n",
    "        \n",
    "        if final_r2 > -0.0024:\n",
    "            improvement = ((final_r2 - (-0.0024)) / abs(-0.0024) * 100)\n",
    "            print(f\"ğŸš€ IMPROVEMENT: {improvement:,.0f}% better!\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  Still below baseline\")\n",
    "            \n",
    "        print(f\"ğŸ“‰ RMSE: {final_rmse:.4f}\")\n",
    "        print(f\"ğŸ“ MAE:  {final_mae:.4f}\")\n",
    "\n",
    "        print(f\"\\nğŸ“ FILES GENERATED:\")\n",
    "        print(f\"   âœ… enhanced_regression_disruption.pkl (improved model)\")\n",
    "        print(f\"   âœ… results/enhanced_regression_predictions.csv (predictions)\")\n",
    "        print(f\"   âœ… results/enhanced_regression_analysis.png (visualizations)\")\n",
    "        print(f\"   âœ… results/model_comparison_enhanced.csv (model comparison)\")\n",
    "\n",
    "        print(f\"\\nğŸ” KEY IMPROVEMENTS MADE:\")\n",
    "        print(f\"   1. Feature engineering: interaction terms for top correlated features\")\n",
    "        print(f\"   2. Target transformation: reduced skewness from {y_train.skew():.2f} to {pd.Series(y_train_transformed).skew():.2f}\")\n",
    "        print(f\"   3. Multiple algorithms tested: Linear, Ridge, Lasso, ElasticNet, RandomForest, GradientBoosting\")\n",
    "        print(f\"   4. Hyperparameter optimization for best performing models\")\n",
    "        print(f\"   5. Cross-validation to ensure robust performance\")\n",
    "\n",
    "        print(f\"\\nğŸ’¡ RECOMMENDATIONS:\")\n",
    "        print(f\"   - Use {final_model_name} for production predictions\")\n",
    "        print(f\"   - Consider ensemble methods for further improvements\")\n",
    "        print(f\"   - Monitor model performance over time\")\n",
    "        print(f\"   - Investigate additional domain-specific feature engineering\")\n",
    "    else:\n",
    "        print(\"âŒ No final model was successfully created\")\n",
    "        \n",
    "    print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95df47a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Analyzing Risk Level Mapping:\n",
      "==================================================\n",
      "ğŸ“Š Risk Level Distribution:\n",
      "risk_classification\n",
      "High Risk        23944\n",
      "Moderate Risk     5011\n",
      "Low Risk          3110\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Disruption Likelihood Score Ranges by Risk Level:\n",
      "                     Count    Mean     Std     Min     Max     Q25  Median  \\\n",
      "risk_classification                                                          \n",
      "High Risk            23944  0.9491  0.0761  0.7001  1.0000  0.9267  0.9902   \n",
      "Low Risk              3110  0.1434  0.0891  0.0000  0.2998  0.0633  0.1416   \n",
      "Moderate Risk         5011  0.5186  0.1155  0.3001  0.6999  0.4219  0.5306   \n",
      "\n",
      "                        Q75  \n",
      "risk_classification          \n",
      "High Risk            0.9997  \n",
      "Low Risk             0.2224  \n",
      "Moderate Risk        0.6199  \n",
      "\n",
      "ğŸ” Threshold Analysis:\n",
      "ğŸ¯ Using Fixed Business Thresholds:\n",
      "   Low Risk:      0.000 - 0.299\n",
      "   Moderate Risk: 0.300 - 0.699\n",
      "   High Risk:     0.700 - 1.000\n",
      "   High threshold: 0.7\n",
      "   Moderate threshold: 0.3\n",
      "\n",
      "ğŸ“Š Baseline Performance with Fixed Thresholds:\n",
      "   Accuracy against original categories: 1.0000 (100.00%)\n",
      "\n",
      "ğŸ“‹ Original vs Fixed Threshold Mapping:\n",
      "fixed_threshold_category  High Risk  Low Risk  Moderate Risk    All\n",
      "original_category                                                  \n",
      "High Risk                     23944         0              0  23944\n",
      "Low Risk                          0      3110              0   3110\n",
      "Moderate Risk                     0         0           5011   5011\n",
      "All                           23944      3110           5011  32065\n",
      "\n",
      "ğŸ“Š Score Distribution Analysis with Fixed Thresholds:\n",
      "   Low Risk (0.0-0.3):  3110 samples ( 9.70%)\n",
      "   Moderate Risk (0.3-0.7):  5011 samples (15.63%)\n",
      "   High Risk (0.7-1.0): 23944 samples (74.67%)\n",
      "\n",
      "âœ… Using Fixed Business Thresholds:\n",
      "   High Risk threshold: 0.7\n",
      "   Moderate Risk threshold: 0.3\n",
      "\n",
      "ğŸ¯ Mapping Model Predictions to Risk Categories:\n",
      "ğŸ“Š Risk Category Classification Results:\n",
      "   Category Accuracy: 0.7430 (74.30%)\n",
      "\n",
      "ğŸ“‹ Confusion Matrix (Risk Categories):\n",
      "                    Pred_Low Risk  Pred_Moderate Risk  Pred_High Risk\n",
      "True_Low Risk                   0                   0             639\n",
      "True_Moderate Risk              0                   0            1009\n",
      "True_High Risk                  0                   0            4765\n",
      "\n",
      "ğŸ“ˆ Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Low Risk       0.00      0.00      0.00       639\n",
      "Moderate Risk       0.00      0.00      0.00      1009\n",
      "    High Risk       0.74      1.00      0.85      4765\n",
      "\n",
      "     accuracy                           0.74      6413\n",
      "    macro avg       0.25      0.33      0.28      6413\n",
      " weighted avg       0.55      0.74      0.63      6413\n",
      "\n",
      "\n",
      "ğŸ“Š Predicted vs Actual Risk Distribution:\n",
      "               True_Distribution  Predicted_Distribution  Difference\n",
      "High Risk                   4765                  6413.0      1648.0\n",
      "Low Risk                     639                     NaN         NaN\n",
      "Moderate Risk               1009                     NaN         NaN\n",
      "\n",
      "ğŸ’¾ Risk category mapping saved to: results/risk_category_predictions.csv\n",
      "\n",
      "ğŸ¯ Risk Level Thresholds for Future Use:\n",
      "   High Risk:     score >= 0.7000\n",
      "   Moderate Risk: 0.3000 <= score < 0.7000\n",
      "   Low Risk:      score < 0.3000\n"
     ]
    }
   ],
   "source": [
    "# ========== 7. RISK LEVEL MAPPING ANALYSIS ==========\n",
    "print(\"ğŸ¯ Analyzing Risk Level Mapping:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load original data to get risk classifications\n",
    "df_original = pd.read_csv(\"data/dynamic_supply_chain_logistics_dataset.csv\")\n",
    "\n",
    "# Analyze the relationship between disruption_likelihood_score and risk_classification\n",
    "print(\"ğŸ“Š Risk Level Distribution:\")\n",
    "risk_counts = df_original['risk_classification'].value_counts()\n",
    "print(risk_counts)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Disruption Likelihood Score Ranges by Risk Level:\")\n",
    "risk_analysis = df_original.groupby('risk_classification')['disruption_likelihood_score'].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max', \n",
    "    lambda x: x.quantile(0.25), \n",
    "    lambda x: x.quantile(0.5), \n",
    "    lambda x: x.quantile(0.75)\n",
    "]).round(4)\n",
    "\n",
    "risk_analysis.columns = ['Count', 'Mean', 'Std', 'Min', 'Max', 'Q25', 'Median', 'Q75']\n",
    "print(risk_analysis)\n",
    "\n",
    "# Find optimal thresholds based on actual data distribution\n",
    "print(f\"\\nğŸ” Threshold Analysis:\")\n",
    "high_risk_scores = df_original[df_original['risk_classification'] == 'High Risk']['disruption_likelihood_score']\n",
    "moderate_risk_scores = df_original[df_original['risk_classification'] == 'Moderate Risk']['disruption_likelihood_score']\n",
    "low_risk_scores = df_original[df_original['risk_classification'] == 'Low Risk']['disruption_likelihood_score']\n",
    "\n",
    "# Use fixed business-defined thresholds\n",
    "print(\"ğŸ¯ Using Fixed Business Thresholds:\")\n",
    "\n",
    "# Define clear, interpretable thresholds\n",
    "high_threshold = 0.7      # High Risk: 0.7 - 1.0\n",
    "moderate_threshold = 0.3  # Moderate Risk: 0.3 - 0.699, Low Risk: 0.0 - 0.299\n",
    "\n",
    "print(f\"   Low Risk:      0.000 - 0.299\")\n",
    "print(f\"   Moderate Risk: 0.300 - 0.699\") \n",
    "print(f\"   High Risk:     0.700 - 1.000\")\n",
    "print(f\"   High threshold: {high_threshold}\")\n",
    "print(f\"   Moderate threshold: {moderate_threshold}\")\n",
    "\n",
    "# Test these thresholds against original data to see baseline performance\n",
    "def evaluate_fixed_thresholds(high_thresh, mod_thresh):\n",
    "    def temp_map_score(score):\n",
    "        if score >= high_thresh:\n",
    "            return \"High Risk\"\n",
    "        elif score >= mod_thresh:\n",
    "            return \"Moderate Risk\"\n",
    "        else:\n",
    "            return \"Low Risk\"\n",
    "    \n",
    "    predicted_categories = [temp_map_score(score) for score in df_original['disruption_likelihood_score']]\n",
    "    accuracy = (df_original['risk_classification'] == predicted_categories).mean()\n",
    "    return accuracy, predicted_categories\n",
    "\n",
    "baseline_accuracy, baseline_predictions = evaluate_fixed_thresholds(high_threshold, moderate_threshold)\n",
    "\n",
    "print(f\"\\nğŸ“Š Baseline Performance with Fixed Thresholds:\")\n",
    "print(f\"   Accuracy against original categories: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Show how original data maps to these thresholds\n",
    "original_to_fixed_mapping = pd.DataFrame({\n",
    "    'original_category': df_original['risk_classification'],\n",
    "    'fixed_threshold_category': baseline_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nğŸ“‹ Original vs Fixed Threshold Mapping:\")\n",
    "mapping_crosstab = pd.crosstab(original_to_fixed_mapping['original_category'], \n",
    "                              original_to_fixed_mapping['fixed_threshold_category'], \n",
    "                              margins=True)\n",
    "print(mapping_crosstab)\n",
    "\n",
    "# Distribution analysis\n",
    "print(f\"\\nğŸ“Š Score Distribution Analysis with Fixed Thresholds:\")\n",
    "score_ranges = {\n",
    "    'Low Risk (0.0-0.3)': len(df_original[df_original['disruption_likelihood_score'] < 0.3]),\n",
    "    'Moderate Risk (0.3-0.7)': len(df_original[(df_original['disruption_likelihood_score'] >= 0.3) & \n",
    "                                               (df_original['disruption_likelihood_score'] < 0.7)]),\n",
    "    'High Risk (0.7-1.0)': len(df_original[df_original['disruption_likelihood_score'] >= 0.7])\n",
    "}\n",
    "\n",
    "total_samples = len(df_original)\n",
    "for range_name, count in score_ranges.items():\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"   {range_name}: {count:5d} samples ({percentage:5.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Using Fixed Business Thresholds:\")\n",
    "print(f\"   High Risk threshold: {high_threshold}\")\n",
    "print(f\"   Moderate Risk threshold: {moderate_threshold}\")\n",
    "\n",
    "# Create mapping function\n",
    "def map_score_to_risk(score):\n",
    "    \"\"\"Map continuous disruption likelihood score to risk category\"\"\"\n",
    "    if score >= high_threshold:\n",
    "        return \"High Risk\"\n",
    "    elif score >= moderate_threshold:\n",
    "        return \"Moderate Risk\"\n",
    "    else:\n",
    "        return \"Low Risk\"\n",
    "\n",
    "# Apply mapping to best model predictions\n",
    "if 'y_pred_final' in locals():\n",
    "    print(f\"\\nğŸ¯ Mapping Model Predictions to Risk Categories:\")\n",
    "    \n",
    "    # Map predictions to risk categories\n",
    "    predicted_risk_categories = [map_score_to_risk(pred) for pred in y_pred_final]\n",
    "    true_risk_categories = [map_score_to_risk(true) for true in y_test]\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    risk_mapping_df = pd.DataFrame({\n",
    "        'y_true_score': y_test,\n",
    "        'y_pred_score': y_pred_final,\n",
    "        'true_risk_category': true_risk_categories,\n",
    "        'predicted_risk_category': predicted_risk_categories\n",
    "    })\n",
    "    \n",
    "    # Calculate category-based accuracy\n",
    "    category_accuracy = (risk_mapping_df['true_risk_category'] == risk_mapping_df['predicted_risk_category']).mean()\n",
    "    \n",
    "    print(f\"ğŸ“Š Risk Category Classification Results:\")\n",
    "    print(f\"   Category Accuracy: {category_accuracy:.4f} ({category_accuracy*100:.2f}%)\")\n",
    "    \n",
    "    # Confusion matrix for risk categories\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    \n",
    "    cm = confusion_matrix(risk_mapping_df['true_risk_category'], \n",
    "                         risk_mapping_df['predicted_risk_category'],\n",
    "                         labels=['Low Risk', 'Moderate Risk', 'High Risk'])\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Confusion Matrix (Risk Categories):\")\n",
    "    categories = ['Low Risk', 'Moderate Risk', 'High Risk']\n",
    "    cm_df = pd.DataFrame(cm, index=[f'True_{cat}' for cat in categories], \n",
    "                        columns=[f'Pred_{cat}' for cat in categories])\n",
    "    print(cm_df)\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nğŸ“ˆ Detailed Classification Report:\")\n",
    "    report = classification_report(risk_mapping_df['true_risk_category'], \n",
    "                                 risk_mapping_df['predicted_risk_category'],\n",
    "                                 labels=['Low Risk', 'Moderate Risk', 'High Risk'])\n",
    "    print(report)\n",
    "    \n",
    "    # Save results\n",
    "    risk_mapping_df.to_csv(\"results/risk_category_predictions.csv\", index=False)\n",
    "    \n",
    "    # Distribution of predicted categories vs actual\n",
    "    print(f\"\\nğŸ“Š Predicted vs Actual Risk Distribution:\")\n",
    "    pred_dist = pd.Series(predicted_risk_categories).value_counts().sort_index()\n",
    "    true_dist = pd.Series(true_risk_categories).value_counts().sort_index()\n",
    "    \n",
    "    dist_comparison = pd.DataFrame({\n",
    "        'True_Distribution': true_dist,\n",
    "        'Predicted_Distribution': pred_dist,\n",
    "        'Difference': pred_dist - true_dist\n",
    "    })\n",
    "    print(dist_comparison)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Risk category mapping saved to: results/risk_category_predictions.csv\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  No final model predictions available yet. Run previous cells first.\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Risk Level Thresholds for Future Use:\")\n",
    "print(f\"   High Risk:     score >= {high_threshold:.4f}\")\n",
    "print(f\"   Moderate Risk: {moderate_threshold:.4f} <= score < {high_threshold:.4f}\")\n",
    "print(f\"   Low Risk:      score < {moderate_threshold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2de2fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing Lasso Model with Processed Features:\n",
      "============================================================\n",
      "ğŸ“‹ Using Processed Data:\n",
      "   Training features: (25652, 38)\n",
      "   Test features: (6413, 38)\n",
      "   Feature engineering applied: âœ…\n",
      "   Enhanced features available: 38\n",
      "\n",
      "ğŸ”§ Enhanced Features in Use:\n",
      "   Original correlation-based: 15\n",
      "   Interaction features: 10\n",
      "   Polynomial features: 13\n",
      "   Total enhanced features: 38\n",
      "\n",
      "ğŸ”¨ Building Optimized Lasso Model...\n",
      "   ğŸ“š Training on enhanced processed features...\n",
      "   ğŸ¯ Making predictions on processed test set...\n",
      "\n",
      "ğŸ“Š Lasso Model Performance (Continuous Scores):\n",
      "   RÂ² Score: -0.000792\n",
      "   RMSE:     0.280182\n",
      "   MAE:      0.224270\n",
      "   Prediction Variance: 0.000066\n",
      "   Unique Predictions:  6413\n",
      "   âœ… Model producing varied predictions\n",
      "\n",
      "ğŸ¯ Mapping Predictions to Risk Categories:\n",
      "   ğŸ“ˆ Using thresholds: High >= 0.7000, Moderate >= 0.3000\n",
      "\n",
      "ğŸ“Š Risk Category Prediction Results:\n",
      "   Accuracy vs Original Categories: 0.7430 (74.30%)\n",
      "   Accuracy vs Score-Derived Categories: 0.7430 (74.30%)\n",
      "\n",
      "ğŸ“‹ Confusion Matrix (vs Original Risk Categories):\n",
      "                    Pred_Low Risk  Pred_Moderate Risk  Pred_High Risk\n",
      "True_Low Risk                   0                   0             639\n",
      "True_Moderate Risk              0                   0            1009\n",
      "True_High Risk                  0                   0            4765\n",
      "\n",
      "ğŸ“ˆ Classification Report (vs Original Categories):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Low Risk       0.00      0.00      0.00       639\n",
      "Moderate Risk       0.00      0.00      0.00      1009\n",
      "    High Risk       0.74      1.00      0.85      4765\n",
      "\n",
      "     accuracy                           0.74      6413\n",
      "    macro avg       0.25      0.33      0.28      6413\n",
      " weighted avg       0.55      0.74      0.63      6413\n",
      "\n",
      "\n",
      "ğŸ“Š Risk Distribution Comparison:\n",
      "               Original_Count  Predicted_Count  Difference  Original_Pct  \\\n",
      "High Risk                4765           6413.0      1648.0         74.30   \n",
      "Low Risk                  639              NaN         NaN          9.96   \n",
      "Moderate Risk            1009              NaN         NaN         15.73   \n",
      "\n",
      "               Predicted_Pct  \n",
      "High Risk              100.0  \n",
      "Low Risk                 NaN  \n",
      "Moderate Risk            NaN  \n",
      "\n",
      "ğŸ” Lasso Coefficient Analysis:\n",
      "   Non-zero coefficients: 24/38\n",
      "   Maximum coefficient magnitude: 0.007122\n",
      "\n",
      "ğŸ† Top 10 Contributing Features:\n",
      "   cargo_condition_status        : 0.007122\n",
      "   order_fulfillment_status_squar: 0.004616\n",
      "   order_fulfillment_status_x_veh: -0.004202\n",
      "   warehouse_inventory_level_squa: 0.003889\n",
      "   port_congestion_level         : 0.003274\n",
      "   delay_probability_x_cargo_cond: -0.003266\n",
      "   warehouse_inventory_level_x_or: -0.003129\n",
      "   warehouse_inventory_level_x_dr: -0.002830\n",
      "   driver_behavior_score         : 0.002346\n",
      "   warehouse_inventory_level_x_ca: -0.001759\n",
      "\n",
      "ğŸ’¾ Detailed results saved to: results/lasso_risk_predictions_detailed.csv\n",
      "\n",
      "ğŸ” Sample Predictions (first 10):\n",
      "   True: 0.9912 (High Risk   ) | Pred: 0.8293 (High Risk   )\n",
      "   True: 1.0000 (High Risk   ) | Pred: 0.8214 (High Risk   )\n",
      "   True: 0.9954 (High Risk   ) | Pred: 0.7924 (High Risk   )\n",
      "   True: 0.9964 (High Risk   ) | Pred: 0.7936 (High Risk   )\n",
      "   True: 0.9917 (High Risk   ) | Pred: 0.8007 (High Risk   )\n",
      "   True: 1.0000 (High Risk   ) | Pred: 0.8011 (High Risk   )\n",
      "   True: 0.7990 (High Risk   ) | Pred: 0.8045 (High Risk   )\n",
      "   True: 0.9970 (High Risk   ) | Pred: 0.8057 (High Risk   )\n",
      "   True: 0.5270 (Moderate Risk) | Pred: 0.7879 (High Risk   )\n",
      "   True: 0.8815 (High Risk   ) | Pred: 0.8046 (High Risk   )\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== 8. LASSO MODEL TESTING & RISK LEVEL EVALUATION ==========\n",
    "print(\"ğŸ§ª Testing Lasso Model with Processed Features:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify we have the processed data available\n",
    "print(f\"ğŸ“‹ Using Processed Data:\")\n",
    "print(f\"   Training features: {X_train_enhanced.shape}\")\n",
    "print(f\"   Test features: {X_test_enhanced.shape}\")\n",
    "print(f\"   Feature engineering applied: âœ…\")\n",
    "print(f\"   Enhanced features available: {len(all_features)}\")\n",
    "\n",
    "# Show the enhanced features being used\n",
    "print(f\"\\nğŸ”§ Enhanced Features in Use:\")\n",
    "print(f\"   Original correlation-based: {len(high_corr_features)}\")\n",
    "print(f\"   Interaction features: {len(interaction_features)}\")\n",
    "print(f\"   Polynomial features: {len(poly_feature_names)}\")\n",
    "print(f\"   Total enhanced features: {len(all_features)}\")\n",
    "\n",
    "# Create and train the optimized Lasso model\n",
    "print(f\"\\nğŸ”¨ Building Optimized Lasso Model...\")\n",
    "lasso_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Lasso(alpha=0.0001, max_iter=2000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on processed features\n",
    "print(\"   ğŸ“š Training on enhanced processed features...\")\n",
    "lasso_model.fit(X_train_enhanced, y_train)\n",
    "\n",
    "# Make predictions\n",
    "print(\"   ğŸ¯ Making predictions on processed test set...\")\n",
    "lasso_predictions = lasso_model.predict(X_test_enhanced)\n",
    "\n",
    "# Check model performance\n",
    "lasso_r2 = r2_score(y_test, lasso_predictions)\n",
    "lasso_rmse = np.sqrt(mean_squared_error(y_test, lasso_predictions))\n",
    "lasso_mae = mean_absolute_error(y_test, lasso_predictions)\n",
    "\n",
    "print(f\"\\nğŸ“Š Lasso Model Performance (Continuous Scores):\")\n",
    "print(f\"   RÂ² Score: {lasso_r2:.6f}\")\n",
    "print(f\"   RMSE:     {lasso_rmse:.6f}\")\n",
    "print(f\"   MAE:      {lasso_mae:.6f}\")\n",
    "\n",
    "# Check prediction variance\n",
    "pred_variance = np.var(lasso_predictions)\n",
    "pred_unique = len(np.unique(lasso_predictions))\n",
    "print(f\"   Prediction Variance: {pred_variance:.6f}\")\n",
    "print(f\"   Unique Predictions:  {pred_unique}\")\n",
    "\n",
    "if pred_unique == 1:\n",
    "    print(\"   âš ï¸  WARNING: Model is predicting constant values!\")\n",
    "    print(f\"   Predicted value: {lasso_predictions[0]:.6f}\")\n",
    "    print(f\"   Target mean:     {y_test.mean():.6f}\")\n",
    "else:\n",
    "    print(f\"   âœ… Model producing varied predictions\")\n",
    "\n",
    "# Map continuous predictions to risk categories\n",
    "print(f\"\\nğŸ¯ Mapping Predictions to Risk Categories:\")\n",
    "predicted_risk_lasso = [map_score_to_risk(pred) for pred in lasso_predictions]\n",
    "actual_risk_lasso = [map_score_to_risk(true) for true in y_test]\n",
    "\n",
    "# Get original risk classifications for test set\n",
    "# We need to map back to original risk classifications from the dataset\n",
    "df_test_with_risk = df.iloc[y_test.index].copy()\n",
    "original_risk_categories = df_test_with_risk['risk_classification'].values\n",
    "\n",
    "print(f\"   ğŸ“ˆ Using thresholds: High >= {high_threshold:.4f}, Moderate >= {moderate_threshold:.4f}\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "lasso_results_df = pd.DataFrame({\n",
    "    'y_true_score': y_test.values,\n",
    "    'y_pred_score': lasso_predictions,\n",
    "    'original_risk_category': original_risk_categories,\n",
    "    'predicted_risk_category': predicted_risk_lasso,\n",
    "    'score_derived_risk': actual_risk_lasso\n",
    "})\n",
    "\n",
    "# Calculate accuracies\n",
    "original_accuracy = (lasso_results_df['original_risk_category'] == lasso_results_df['predicted_risk_category']).mean()\n",
    "score_accuracy = (lasso_results_df['score_derived_risk'] == lasso_results_df['predicted_risk_category']).mean()\n",
    "\n",
    "print(f\"\\nğŸ“Š Risk Category Prediction Results:\")\n",
    "print(f\"   Accuracy vs Original Categories: {original_accuracy:.4f} ({original_accuracy*100:.2f}%)\")\n",
    "print(f\"   Accuracy vs Score-Derived Categories: {score_accuracy:.4f} ({score_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Detailed analysis - Original risk categories\n",
    "print(f\"\\nğŸ“‹ Confusion Matrix (vs Original Risk Categories):\")\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm_original = confusion_matrix(lasso_results_df['original_risk_category'], \n",
    "                              lasso_results_df['predicted_risk_category'],\n",
    "                              labels=['Low Risk', 'Moderate Risk', 'High Risk'])\n",
    "\n",
    "categories = ['Low Risk', 'Moderate Risk', 'High Risk']\n",
    "cm_df_original = pd.DataFrame(cm_original, \n",
    "                             index=[f'True_{cat}' for cat in categories], \n",
    "                             columns=[f'Pred_{cat}' for cat in categories])\n",
    "print(cm_df_original)\n",
    "\n",
    "# Classification report for original categories\n",
    "print(f\"\\nğŸ“ˆ Classification Report (vs Original Categories):\")\n",
    "report_original = classification_report(lasso_results_df['original_risk_category'], \n",
    "                                       lasso_results_df['predicted_risk_category'],\n",
    "                                       labels=['Low Risk', 'Moderate Risk', 'High Risk'],\n",
    "                                       zero_division=0)\n",
    "print(report_original)\n",
    "\n",
    "# Distribution comparison\n",
    "print(f\"\\nğŸ“Š Risk Distribution Comparison:\")\n",
    "original_dist = pd.Series(lasso_results_df['original_risk_category']).value_counts().sort_index()\n",
    "predicted_dist = pd.Series(lasso_results_df['predicted_risk_category']).value_counts().sort_index()\n",
    "\n",
    "dist_comparison = pd.DataFrame({\n",
    "    'Original_Count': original_dist,\n",
    "    'Predicted_Count': predicted_dist,\n",
    "    'Difference': predicted_dist - original_dist,\n",
    "    'Original_Pct': (original_dist / len(lasso_results_df) * 100).round(2),\n",
    "    'Predicted_Pct': (predicted_dist / len(lasso_results_df) * 100).round(2)\n",
    "})\n",
    "print(dist_comparison)\n",
    "\n",
    "# Analyze coefficient values\n",
    "print(f\"\\nğŸ” Lasso Coefficient Analysis:\")\n",
    "lasso_regressor = lasso_model.named_steps['regressor']\n",
    "non_zero_coefs = np.sum(np.abs(lasso_regressor.coef_) > 1e-10)\n",
    "total_coefs = len(lasso_regressor.coef_)\n",
    "max_coef = np.max(np.abs(lasso_regressor.coef_))\n",
    "\n",
    "print(f\"   Non-zero coefficients: {non_zero_coefs}/{total_coefs}\")\n",
    "print(f\"   Maximum coefficient magnitude: {max_coef:.6f}\")\n",
    "\n",
    "if non_zero_coefs > 0:\n",
    "    # Show top contributing features\n",
    "    coef_importance = pd.DataFrame({\n",
    "        'feature': all_features,\n",
    "        'coefficient': lasso_regressor.coef_,\n",
    "        'abs_coefficient': np.abs(lasso_regressor.coef_)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    top_features = coef_importance[coef_importance['abs_coefficient'] > 1e-10].head(10)\n",
    "    print(f\"\\nğŸ† Top {len(top_features)} Contributing Features:\")\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"   {row['feature'][:30]:30s}: {row['coefficient']:8.6f}\")\n",
    "else:\n",
    "    print(\"   âš ï¸  All coefficients are zero - model is not learning!\")\n",
    "\n",
    "# Save detailed results\n",
    "lasso_results_df.to_csv(\"results/lasso_risk_predictions_detailed.csv\", index=False)\n",
    "print(f\"\\nğŸ’¾ Detailed results saved to: results/lasso_risk_predictions_detailed.csv\")\n",
    "\n",
    "# Sample predictions for inspection\n",
    "print(f\"\\nğŸ” Sample Predictions (first 10):\")\n",
    "sample_results = lasso_results_df.head(10)[['y_true_score', 'y_pred_score', 'original_risk_category', 'predicted_risk_category']]\n",
    "for idx, row in sample_results.iterrows():\n",
    "    print(f\"   True: {row['y_true_score']:.4f} ({row['original_risk_category']:12s}) | \"\n",
    "          f\"Pred: {row['y_pred_score']:.4f} ({row['predicted_risk_category']:12s})\")\n",
    "\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "business_risk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
